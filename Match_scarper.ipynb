{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":30067,"databundleVersionId":2628055,"sourceType":"competition"},{"sourceId":9779329,"sourceType":"datasetVersion","datasetId":9}],"dockerImageVersionId":30178,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"This notebook downloads episodes using Kaggle's GetEpisodeReplay API and the [Meta Kaggle](https://www.kaggle.com/kaggle/meta-kaggle) dataset.\n\n**To run this notebook you WILL need to re-add the Meta Kaggle dataset. After opening your copy of the notebook, click \"+ Add data\" top right in the notebook editor.\n**\n\nMeta Kaggle is refreshed daily, but sometimes misses daily refreshes for a few days.\n\nWhy download replays?\n- Train your ML/RL model\n- Inspect the performance of yours and others agents\n- To add to your ever growing json collection \n\nOnly one scraping strategy is implemented: For each top scoring submission, download all missing matches, move on to next submission.\n\nOther scraping strategies can be implemented, but not here. Like download max X matches per submission or per team per day, or ignore certain teams or ignore where some scores < X, or only download some teams.\n\nTodo:\n- Add teamid's once meta kaggle add them. Edit: it's been a long time, it doesn't look like Kaggle is adding this.","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport os\nimport requests\nimport json\nimport datetime\nimport time\nimport glob\nimport collections\n\n## You should configure these to your needs. Choose one of ...\n# 'hungry-geese', 'rock-paper-scissors', santa-2020', 'halite', 'google-football'\nCOMP = 'lux-ai-2021'\nMAX_CALLS_PER_DAY = 2000 # Kaggle says don't do more than 3600 per day and 1 per second\n\nMATCH_DIR = '../working/'\nbase_url = \"https://www.kaggle.com/requests/EpisodeService/\"\nget_url = base_url + \"GetEpisodeReplay\"\nBUFFER = 1\nCOMPETITIONS = {\n    'lux-ai-2021': 30067,\n    'hungry-geese': 25401,\n    'rock-paper-scissors': 22838,\n    'santa-2020': 24539,\n    'halite': 18011,\n    'google-football': 21723\n}\n\n\n# RELOAD\nsub_to_score_top = [23297953,23281649,23692494]\n\n\n\n# Filter Episodes.csv\ndata = pd.read_csv(META + \"Episodes.csv\", chunksize=1e6)\ndf_list = [] \nfor chunk in data:\n    df_list.append(chunk[chunk['CompetitionId']==COMPETITIONS[COMP]])\nepisodes_df = pd.concat(df_list)\ndel data\ndel chunk\ndel df_list\nprint(f'Episodes.csv: {len(episodes_df)} rows after filtering for {COMP}.')\n\n\n\n# Filter EpisodeAgents.csv\ndata = pd.read_csv(META + \"EpisodeAgents.csv\", chunksize=1e6)\ndf_list = [] \nfor chunk in data:\n    df_list.append(chunk[chunk.EpisodeId.isin(episodes_df.Id)])\nepagents_df = pd.concat(df_list)\ndel data\ndel chunk\ndel df_list\nprint(f'EpisodeAgents.csv: {len(epagents_df)} rows after filtering for {COMP}.')\n\n\n#filter for toad\nBASE_OUTPUT_DIRECTORY = '/kaggle/working/episodes'\nSUFFIX_DIRECTORY = 'TB'\nprint(f'EpisodeAgents.csv: {len(epagents_df)} rows before filtering for {sub_to_score_top}.')\nepagents_df = epagents_df[epagents_df.SubmissionId.isin(sub_to_score_top)]\nprint(f'EpisodeAgents.csv: {len(epagents_df)} rows after filtering for {sub_to_score_top}.')\n\nprint(f'Episodes.csv: {len(episodes_df)} rows before filtering.')\nepisodes_df = episodes_df[episodes_df.Id.isin(epagents_df.EpisodeId)]\nprint(f'Episodes.csv: {len(episodes_df)} rows after filtering.')\n\n\n\n\n# Prepare dataframes\nepisodes_df = episodes_df.set_index(['Id'])\nepisodes_df['CreateTime'] = pd.to_datetime(episodes_df['CreateTime'])\nepisodes_df['EndTime'] = pd.to_datetime(episodes_df['EndTime'])\n\nepagents_df.fillna(0, inplace=True)\nepagents_df = epagents_df.sort_values(by=['Id'], ascending=False)\n\n\n# Get episodes for these submissions\nprint('Get episodes for these submissions')\nsub_to_episodes = collections.defaultdict(list)\nfor key in sub_to_score_top:\n    eps = sorted(epagents_df[epagents_df['SubmissionId'].isin([key])]['EpisodeId'].values, reverse=True)\n    sub_to_episodes[key] = eps\n\ncandidates = len(set([item for sublist in sub_to_episodes.values() for item in sublist]))\nprint(f'{candidates} episodes for these {len(sub_to_score_top)} submissions')\n","metadata":{"execution":{"iopub.status.busy":"2024-11-01T15:35:22.373946Z","iopub.execute_input":"2024-11-01T15:35:22.374244Z","iopub.status.idle":"2024-11-01T15:43:29.162977Z","shell.execute_reply.started":"2024-11-01T15:35:22.374210Z","shell.execute_reply":"2024-11-01T15:43:29.161787Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\nglobal num_api_calls_today\nnum_api_calls_today = 0\nall_files = []\nfor root, dirs, files in os.walk(MATCH_DIR, topdown=False):\n    all_files.extend(files)\nseen_episodes = [int(f.split('.')[0]) for f in all_files\n                      if '.' in f and f.split('.')[0].isdigit() and f.split('.')[1] == 'json']\nremaining = np.setdiff1d([item for sublist in sub_to_episodes.values() for item in sublist],seen_episodes)\nprint(f'{len(remaining)} of these {candidates} episodes not yet saved')\nprint('Total of {} games in existing library'.format(len(seen_episodes)))\n\n\ndef create_info_json(epid):\n    create_seconds = int((episodes_df[episodes_df.index == epid]['CreateTime'].values[0]).item() / 1e9)\n    end_seconds = int((episodes_df[episodes_df.index == epid]['CreateTime'].values[0]).item() / 1e9)\n\n    agents = []\n    for index, row in epagents_df[epagents_df['EpisodeId'] == epid].sort_values(by=['Index']).iterrows():\n        agent = {\n            \"id\": int(row[\"Id\"]),\n            \"state\": int(row[\"State\"]),\n            \"submissionId\": int(row['SubmissionId']),\n            \"reward\": int(row['Reward']),\n            \"index\": int(row['Index']),\n            \"initialScore\": float(row['InitialScore']),\n            \"initialConfidence\": float(row['InitialConfidence']),\n            \"updatedScore\": float(row['UpdatedScore']),\n            \"updatedConfidence\": float(row['UpdatedConfidence']),\n            \"teamId\": int(99999)\n        }\n        agents.append(agent)\n\n    info = {\n        \"id\": int(epid),\n        \"competitionId\": int(COMPETITIONS[COMP]),\n        \"createTime\": {\n            \"seconds\": int(create_seconds)\n        },\n        \"endTime\": {\n            \"seconds\": int(end_seconds)\n        },\n        \"agents\": agents\n    }\n\n    return info\n\ndef saveEpisode(epid):\n    # request\n    re = requests.post(get_url, json = {\"episodeId\": int(epid)})\n        \n    with open('/kaggle/working/episodesTB' + '{}.json'.format(epid), 'w') as f:\n        json.dump(re.json(), f)  # Converts dict to JSON string and writes to file\n    return True \n\n\n\ndef get_path(directory,epid):\n    return '{}/{}.json'.format(directory,epid)\n\nr = BUFFER;\n\nstart_time = datetime.datetime.now()\nse = 0\nfor key in sub_to_score_top:\n    if num_api_calls_today <= MAX_CALLS_PER_DAY:\n        print('')\n        remaining = sorted(np.setdiff1d(sub_to_episodes[key], seen_episodes), reverse=True)\n        print(\n            f'submission={key}, matches={len(set(sub_to_episodes[key]))}, still to save={len(remaining)}')\n\n        for epid in remaining:\n            if epid not in seen_episodes and num_api_calls_today <= MAX_CALLS_PER_DAY:\n                if not saveEpisode(epid):\n                    continue # file already existed, we have not used our API call, we do not need to check, next\n                r += 1\n                se += 1\n                try:\n                    print(str(num_api_calls_today) + f': saved episode #{epid}')\n                    seen_episodes.append(epid)\n                    num_api_calls_today += 1\n                except:\n                    print('  file {}.json did not seem to save'.format(epid),':', get_path(directory,epid))\n                if r > (datetime.datetime.now() - start_time).seconds:\n                    time.sleep(r - (datetime.datetime.now() - start_time).seconds)\n            if num_api_calls_today > (min(3600, MAX_CALLS_PER_DAY)):\n                break\nprint('')\nprint(f'Episodes saved: {se}')\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport os\nimport requests\nimport json\nimport datetime\nimport time\nimport glob\nimport collections","metadata":{"execution":{"iopub.status.busy":"2024-11-01T15:34:12.037411Z","iopub.execute_input":"2024-11-01T15:34:12.038395Z","iopub.status.idle":"2024-11-01T15:34:12.043635Z","shell.execute_reply.started":"2024-11-01T15:34:12.038337Z","shell.execute_reply":"2024-11-01T15:34:12.042632Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"## You should configure these to your needs. Choose one of ...\n# 'hungry-geese', 'rock-paper-scissors', santa-2020', 'halite', 'google-football'\nCOMP = 'lux-ai-2021'\nMAX_CALLS_PER_DAY = 3300 # Kaggle says don't do more than 3600 per day and 1 per second\nLOWEST_SCORE_THRESH = 1800","metadata":{"execution":{"iopub.status.busy":"2024-11-01T15:47:12.384353Z","iopub.execute_input":"2024-11-01T15:47:12.384748Z","iopub.status.idle":"2024-11-01T15:47:12.394843Z","shell.execute_reply.started":"2024-11-01T15:47:12.384702Z","shell.execute_reply":"2024-11-01T15:47:12.391528Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"ROOT =\"../working/\"\nMETA = \"../input/meta-kaggle/\"\nMATCH_DIR = '../working/'\n#base_url = \"https://www.kaggle.com/requests/EpisodeService/\"\nbase_url = \"https://www.kaggle.com/api/i/competitions.EpisodeService/\"\n    \nget_url = base_url + \"GetEpisodeReplay\"\nBUFFER = 1\nCOMPETITIONS = {\n    'kore-2022': 34419,\n    'lux-ai-2021': 30067,\n    'hungry-geese': 25401,\n    'rock-paper-scissors': 22838,\n    'santa-2020': 24539,\n    'halite': 18011,\n    'google-football': 21723\n}","metadata":{"execution":{"iopub.status.busy":"2024-11-01T15:47:14.538505Z","iopub.execute_input":"2024-11-01T15:47:14.538817Z","iopub.status.idle":"2024-11-01T15:47:14.546824Z","shell.execute_reply.started":"2024-11-01T15:47:14.538782Z","shell.execute_reply":"2024-11-01T15:47:14.545322Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Filter Episodes.csv\ndata = pd.read_csv(META + \"Episodes.csv\", chunksize=1e6)\ndf_list = [] \nfor chunk in data:\n    df_list.append(chunk[chunk['CompetitionId']==COMPETITIONS[COMP]])\nepisodes_df = pd.concat(df_list)\ndel data\ndel chunk\ndel df_list\nprint(f'Episodes.csv: {len(episodes_df)} rows after filtering for {COMP}.')","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Filter EpisodeAgents.csv\ndata = pd.read_csv(META + \"EpisodeAgents.csv\", chunksize=1e6)\ndf_list = [] \nfor chunk in data:\n    df_list.append(chunk[chunk.EpisodeId.isin(episodes_df.Id)])\nepagents_df = pd.concat(df_list)\ndel data\ndel chunk\ndel df_list\nprint(f'EpisodeAgents.csv: {len(epagents_df)} rows after filtering for {COMP}.')","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Prepare dataframes\n\nepisodes_df = episodes_df.set_index(['Id'])\nepisodes_df['CreateTime'] = pd.to_datetime(episodes_df['CreateTime'])\nepisodes_df['EndTime'] = pd.to_datetime(episodes_df['EndTime'])\n\nepagents_df.fillna(0, inplace=True)\nepagents_df = epagents_df.sort_values(by=['Id'], ascending=False)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Get top scoring submissions# Get top scoring submissions\nmax_df = (epagents_df.sort_values(by=['EpisodeId'], ascending=False).groupby('SubmissionId').head(1).drop_duplicates().reset_index(drop=True))\nmax_df = max_df[max_df.UpdatedScore>=LOWEST_SCORE_THRESH]\nmax_df = pd.merge(left=episodes_df, right=max_df, left_on='Id', right_on='EpisodeId')\nsub_to_score_top = pd.Series(max_df.UpdatedScore.values,index=max_df.SubmissionId).to_dict()\nprint(f'{len(sub_to_score_top)} submissions with score over {LOWEST_SCORE_THRESH}')","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Get episodes for these submissions\nsub_to_episodes = collections.defaultdict(list)\nfor key, value in sorted(sub_to_score_top.items(), key=lambda kv: kv[1], reverse=True):\n    excl = []\n    if key not in excl: # we can filter subs like this\n        eps = sorted(epagents_df[epagents_df['SubmissionId'].isin([key])]['EpisodeId'].values,reverse=True)\n        sub_to_episodes[key] = eps\ncandidates = len(set([item for sublist in sub_to_episodes.values() for item in sublist]))\nprint(f'{candidates} episodes for these {len(sub_to_score_top)} submissions')","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"global num_api_calls_today\nnum_api_calls_today = 0\nall_files = []\nfor root, dirs, files in os.walk(MATCH_DIR, topdown=False):\n    all_files.extend(files)\nseen_episodes = [int(f.split('.')[0]) for f in all_files \n                      if '.' in f and f.split('.')[0].isdigit() and f.split('.')[1] == 'json']\nremaining = np.setdiff1d([item for sublist in sub_to_episodes.values() for item in sublist],seen_episodes)\nprint(f'{len(remaining)} of these {candidates} episodes not yet saved')\nprint('Total of {} games in existing library'.format(len(seen_episodes)))","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def create_info_json(epid):\n    \n    create_seconds = int((episodes_df[episodes_df.index == epid]['CreateTime'].values[0]).item()/1e9)\n    end_seconds = int((episodes_df[episodes_df.index == epid]['CreateTime'].values[0]).item()/1e9)\n\n    agents = []\n    for index, row in epagents_df[epagents_df['EpisodeId'] == epid].sort_values(by=['Index']).iterrows():\n        agent = {\n            \"id\": int(row[\"Id\"]),\n            \"state\": int(row[\"State\"]),\n            \"submissionId\": int(row['SubmissionId']),\n            \"reward\": int(row['Reward']),\n            \"index\": int(row['Index']),\n            \"initialScore\": float(row['InitialScore']),\n            \"initialConfidence\": float(row['InitialConfidence']),\n            \"updatedScore\": float(row['UpdatedScore']),\n            \"updatedConfidence\": float(row['UpdatedConfidence']),\n            \"teamId\": int(99999)\n        }\n        agents.append(agent)\n\n    info = {\n        \"id\": int(epid),\n        \"competitionId\": int(COMPETITIONS[COMP]),\n        \"createTime\": {\n            \"seconds\": int(create_seconds)\n        },\n        \"endTime\": {\n            \"seconds\": int(end_seconds)\n        },\n        \"agents\": agents\n    }\n\n    return info","metadata":{"execution":{"iopub.status.busy":"2024-11-01T14:49:53.171317Z","iopub.execute_input":"2024-11-01T14:49:53.172758Z","iopub.status.idle":"2024-11-01T14:49:53.186969Z","shell.execute_reply.started":"2024-11-01T14:49:53.172705Z","shell.execute_reply":"2024-11-01T14:49:53.185871Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def saveEpisode(epid):\n    # request\n    re = requests.post(get_url, json = {\"episodeId\": int(epid)})\n        \n    with open(MATCH_DIR + '{}.json'.format(epid), 'w') as f:\n        json.dump(re.json(), f)  # Converts dict to JSON string and writes to file\n        print(re.json())\n\n    # save match info\n    info = create_info_json(epid)\n    with open(MATCH_DIR +  '{}_info.json'.format(epid), 'w') as f:\n        json.dump(info, f)\nsaveEpisode(35144875)","metadata":{"execution":{"iopub.status.busy":"2024-11-01T15:48:54.945436Z","iopub.execute_input":"2024-11-01T15:48:54.946589Z","iopub.status.idle":"2024-11-01T15:48:56.134321Z","shell.execute_reply.started":"2024-11-01T15:48:54.946515Z","shell.execute_reply":"2024-11-01T15:48:56.132755Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"r = BUFFER;\n\nstart_time = datetime.datetime.now()\nse=0\nfor key, value in sorted(sub_to_score_top.items(), key=lambda kv: kv[1], reverse=True):\n    if num_api_calls_today<=MAX_CALLS_PER_DAY:\n        print('')\n        remaining = sorted(np.setdiff1d(sub_to_episodes[key],seen_episodes), reverse=True)\n        print(f'submission={key}, LB={\"{:.0f}\".format(value)}, matches={len(set(sub_to_episodes[key]))}, still to save={len(remaining)}')\n        \n        for epid in remaining:\n            if epid not in seen_episodes and num_api_calls_today<=MAX_CALLS_PER_DAY:\n                saveEpisode(epid); \n                r+=1;\n                se+=1\n                try:\n                    size = os.path.getsize(MATCH_DIR+'{}.json'.format(epid)) / 1e6\n                    print(str(num_api_calls_today) + f': saved episode #{epid}')\n                    seen_episodes.append(epid)\n                    num_api_calls_today+=1\n                except:\n                    print('  file {}.json did not seem to save'.format(epid))    \n                if r > (datetime.datetime.now() - start_time).seconds:\n                    time.sleep( r - (datetime.datetime.now() - start_time).seconds)\n            if num_api_calls_today>(min(3600,MAX_CALLS_PER_DAY)):\n                break\nprint('')\nprint(f'Episodes saved: {se}')","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import csv\nwith open(\"output.csv\", mode=\"w\", newline=\"\") as file:\n    writer = csv.writer(file)\n    for item in seen_episodes:\n        writer.writerow([item])","metadata":{"execution":{"iopub.status.busy":"2024-11-01T17:16:24.467424Z","iopub.execute_input":"2024-11-01T17:16:24.470283Z","iopub.status.idle":"2024-11-01T17:16:24.526509Z","shell.execute_reply.started":"2024-11-01T17:16:24.470003Z","shell.execute_reply":"2024-11-01T17:16:24.523879Z"},"trusted":true},"outputs":[],"execution_count":40},{"cell_type":"code","source":"!zip -r file.zip /kaggle/working ","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from IPython.display import FileLink\nFileLink(r'file.zip')","metadata":{"execution":{"iopub.status.busy":"2024-11-01T17:12:25.460065Z","iopub.execute_input":"2024-11-01T17:12:25.460445Z","iopub.status.idle":"2024-11-01T17:12:25.469055Z","shell.execute_reply.started":"2024-11-01T17:12:25.460405Z","shell.execute_reply":"2024-11-01T17:12:25.468010Z"},"trusted":true},"outputs":[{"execution_count":37,"output_type":"execute_result","data":{"text/plain":"/kaggle/working/file.zip","text/html":"<a href='file.zip' target='_blank'>file.zip</a><br>"},"metadata":{}}],"execution_count":37},{"cell_type":"code","source":"!rm -r /kaggle/working/","metadata":{"execution":{"iopub.status.busy":"2024-11-01T17:16:11.121667Z","iopub.execute_input":"2024-11-01T17:16:11.122036Z","iopub.status.idle":"2024-11-01T17:16:13.088236Z","shell.execute_reply.started":"2024-11-01T17:16:11.121998Z","shell.execute_reply":"2024-11-01T17:16:13.087189Z"},"trusted":true},"outputs":[{"name":"stdout","text":"rm: cannot remove '/kaggle/working/': Device or resource busy\n","output_type":"stream"}],"execution_count":39}]}